{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Suman_file2_hw2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-6lVvA96_I8"
      },
      "source": [
        "## ANALYSIS WITH FULL DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbX0m3bRlRqz"
      },
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import tarfile"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61M0zgukllro",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7fe4d1ff-dbaa-4821-aef3-5fd1a2d48dc5"
      },
      "source": [
        "basepath= '/content/drive/MyDrive/Colab_Notebooks/nlpAssignment'\n",
        "folder= Path(basepath)\n",
        "folder"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/Colab_Notebooks/nlpAssignment')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L1hInMHlqiq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a2a26675-4e1c-4c49-d118-e9e8331995be"
      },
      "source": [
        "#The spam csv has already been downloaded and saved in the data folder.\n",
        "spam = folder / 'spam.csv'\n",
        "!head {spam}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v1,v2,,,\r\n",
            "ham,\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\",,,\r\n",
            "ham,Ok lar... Joking wif u oni...,,,\r\n",
            "spam,Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's,,,\r\n",
            "ham,U dun say so early hor... U c already then say...,,,\r\n",
            "ham,\"Nah I don't think he goes to usf, he lives around here though\",,,\r\n",
            "spam,\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, �1.50 to rcv\",,,\r\n",
            "ham,Even my brother is not like to speak with me. They treat me like aids patent.,,,\r\n",
            "ham,As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune,,,\r\n",
            "spam,WINNER!! As a valued network customer you have been selected to receivea �900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.,,,\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OdUUc-l10q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "07dae500-dad0-400e-a634-9889d19b6f90"
      },
      "source": [
        "#Load Dataset\n",
        "# convert to pandas dataframe\n",
        "spam_file = pd.read_csv(spam,encoding='ISO-8859-1',\n",
        "                        names= ['label','message','1','2','3'],\n",
        "                        usecols=['label','message'], header=0)\n",
        "spam_df = pd.DataFrame(spam_file)\n",
        "spam_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcF1nHmcl76L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6b424342-6f62-4a1d-f361-7de462456150"
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 61.5 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 44.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Collecting thinc<8.1.0,>=8.0.9\n",
            "  Downloading thinc-8.0.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (623 kB)\n",
            "\u001b[K     |████████████████████████████████| 623 kB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.0 pydantic-1.8.2 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M40BRvDSl_85"
      },
      "source": [
        "import spacy\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T25Npg6mDmT"
      },
      "source": [
        "spacy_folder = Path('/content/drive/MyDrive/Colab_Notebooks/nlpAssignment/SPACY')\n",
        "model = spacy_folder /'en_core_web_sm-3.1.0'/'en_core_web_sm'/'en_core_web_sm-3.1.0'\n",
        "nlp = spacy.load(model, disable=['parser'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcYKeL3F8GLU"
      },
      "source": [
        "Custom Spacy Preprocessor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8wSNhPCmJgc"
      },
      "source": [
        "class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    np.random.seed(0)\n",
        "    def __init__(self, lammetize=True, lower=True, remove_stop=True, \n",
        "                 remove_punct=True, remove_email=True, remove_url=True, \n",
        "                 remove_num=False, stemming = False,\n",
        "                 add_user_mention_prefix=True, remove_hashtag_prefix=False):\n",
        "        self.remove_stop = remove_stop\n",
        "        self.remove_punct = remove_punct\n",
        "        self.remove_num = remove_num\n",
        "        self.remove_url = remove_url\n",
        "        self.remove_email = remove_email\n",
        "        self.lammetize = lammetize\n",
        "        self.lower = lower\n",
        "        self.stemming = stemming\n",
        "        self.add_user_mention_prefix = add_user_mention_prefix\n",
        "        self.remove_hashtag_prefix = remove_hashtag_prefix\n",
        "\n",
        " # helpfer functions for basic cleaning \n",
        "\n",
        "    def basic_clean(self, text):\n",
        "        \n",
        "        '''\n",
        "        This fuction removes HTML tags from text\n",
        "        '''\n",
        "        if (bool(BeautifulSoup(text, \"html.parser\").find())==True):         \n",
        "            soup = BeautifulSoup(text, \"html.parser\")\n",
        "            text = soup.get_text()\n",
        "        else:\n",
        "            pass\n",
        "        return re.sub(r'[\\n\\r]',' ', text) \n",
        "\n",
        "    # helper function for pre-processing with spacy and Porter Stemmer\n",
        "    \n",
        "    def spacy_preprocessor(self,texts):\n",
        "\n",
        "        final_result = []\n",
        "        nlp = spacy.load(model, disable=['parser','ner'])\n",
        "        \n",
        "        ## Add @ as a prefix so that we can separate the word from its token\n",
        "        prefixes = list(nlp.Defaults.prefixes)\n",
        "\n",
        "        if self.add_user_mention_prefix:\n",
        "            prefixes += ['@']\n",
        "\n",
        "        ## Remove # as a prefix so that we can keep hashtags and words together\n",
        "        if self.remove_hashtag_prefix:\n",
        "            prefixes.remove(r'#')\n",
        "\n",
        "        prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n",
        "        nlp.tokenizer.prefix_search = prefix_regex.search\n",
        "\n",
        "        matcher = Matcher(nlp.vocab)\n",
        "        if self.remove_stop:\n",
        "            matcher.add(\"stop_words\", [[{\"is_stop\" : True}]])\n",
        "        if self.remove_punct:\n",
        "            matcher.add(\"punctuation\",[ [{\"is_punct\": True}]])\n",
        "        if self.remove_num:\n",
        "            matcher.add(\"numbers\", [[{\"like_num\": True}]])\n",
        "        if self.remove_url:\n",
        "            matcher.add(\"urls\", [[{\"like_url\": True}]])\n",
        "        if self.remove_email:\n",
        "            matcher.add(\"emails\", [[{\"like_email\": True}]])\n",
        "            \n",
        "        Token.set_extension('is_remove', default=False, force=True)\n",
        "\n",
        "        cleaned_text = []\n",
        "        for doc in nlp.pipe(texts,batch_size=64,disable=['parser','ner']):\n",
        "            matches = matcher(doc)\n",
        "            for _, start, end in matches:\n",
        "                for token in doc[start:end]:\n",
        "                    token._.is_remove =True\n",
        "                    \n",
        "            if self.lammetize:              \n",
        "                text = ' '.join(token.lemma_ for token in doc \n",
        "                                if (token._.is_remove==False))\n",
        "            elif self.stemming:\n",
        "                text =' '.join(PorterStemmer().stem(token.text) for token in doc \n",
        "                                if (token._.is_remove==False))\n",
        "            else:\n",
        "                text = ' '.join(token.text for token in doc \n",
        "                                if (token._.is_remove==False))\n",
        "                                   \n",
        "            if self.lower:\n",
        "                text=text.lower()\n",
        "            cleaned_text.append(text)\n",
        "        return cleaned_text\n",
        "\n",
        "    def fit(self, X,y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        try:\n",
        "            if str(type(X)) not in [\"<class 'list'>\",\"<class 'numpy.ndarray'>\"]:\n",
        "                raise Exception('Expected list or numpy array got {}'.format(type(X)))\n",
        "            x_clean = [self.basic_clean(text) for text in X]\n",
        "            x_clean_final = self.spacy_preprocessor(x_clean)\n",
        "            return x_clean_final\n",
        "        except Exception as error:\n",
        "            print('An exception occured: ' + repr(error))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cth0JCswmL_O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "da9aba49-a7e1-44f9-9cea-dfc2bcb416fe"
      },
      "source": [
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "import re"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.2-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyFPcxapn2wL"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer \n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Token\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceWDG7Sb8NEQ"
      },
      "source": [
        "Custom Feature Engineering Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py4DOWl1mTSE"
      },
      "source": [
        "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
        "    np.random.seed(0)\n",
        "    def __init__(self, word_count=False, char_count=False, \n",
        "                 char_count_wo_space=False, \n",
        "                 avg_word_length=True, digit_count=True,\n",
        "                 punc_count= True, spell_check= True):\n",
        "        self.word_count = word_count\n",
        "        self.char_count = char_count\n",
        "        self.char_count_wo_space = char_count_wo_space\n",
        "        self.avg_word_length = avg_word_length\n",
        "        self.digit_count = digit_count\n",
        "        self.punc_count = punc_count\n",
        "        self.spell_check = spell_check\n",
        "        self.spell = SpellChecker()\n",
        "\n",
        "    def fit(self, X,y=None):\n",
        "        return self\n",
        "\n",
        "    #Useful functions\n",
        "\n",
        "    def wordCount(self,text):\n",
        "        return len(text.split())\n",
        "\n",
        "    def charCount(self,text):\n",
        "        return len(text)\n",
        "\n",
        "    def charCountWithoutSpace(self,text):\n",
        "        count = 0\n",
        "        for word in text.split():\n",
        "            count += len(word)\n",
        "        return count\n",
        "\n",
        "    def avgWordLength(self,text):\n",
        "        word_length = 0\n",
        "        for token in text.split():\n",
        "            word_length += len(token)\n",
        "        word_count = len(text.split())\n",
        "        if word_count == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return word_length/word_count\n",
        "\n",
        "    def digitCount(self,text):\n",
        "        count = 0\n",
        "        for i in text:\n",
        "            if i.isdigit():\n",
        "                count += 1\n",
        "        return count\n",
        "    \n",
        "    def punctuationCount(self,text):\n",
        "      line = re.findall(r'[!\"$%&\\'()*+,-./:;=#@?[\\]^_`{|}~]*', text)     \n",
        "      string=\"\".join(line)     \n",
        "      return len(list(string))\n",
        "      # puncList = [token for token in self.nlpdocs if(token.is_punct)]\n",
        "      # return len(puncList)\n",
        "\n",
        "    def spellcheck(self, text,spell):\n",
        "      misspelled= spell.unknown(text.split())\n",
        "      return len(misspelled)\n",
        "\n",
        "    def transform(self, X,y=None):\n",
        "        try:\n",
        "            if str(type(X)) not in [\"<class 'list'>\",\"<class 'numpy.ndarray'>\"]:\n",
        "                raise Exception('Expected list or numpy array got {}'.format(type(X)))\n",
        "            final_result = []\n",
        "            for index,item in enumerate(X):\n",
        "                self.nlpdocs = None\n",
        "                res = []\n",
        "                if self.word_count:\n",
        "                    res.append(self.wordCount(item))\n",
        "                if self.char_count:\n",
        "                    res.append(self.charCount(item))\n",
        "                if self.char_count_wo_space:\n",
        "                    res.append(self.charCountWithoutSpace(item))\n",
        "                if self.avg_word_length:\n",
        "                    res.append(self.avgWordLength(item))\n",
        "                if self.digit_count:\n",
        "                    res.append(self.digitCount(item))\n",
        "                if self.punc_count:\n",
        "                    res.append(self.punctuationCount(item))\n",
        "                if self.spell_check:\n",
        "                    res.append(self.spellcheck(item,self.spell))\n",
        "\n",
        "                final_result.append(res)\n",
        "            return np.array(final_result)\n",
        "        except Exception as error:\n",
        "            print('An exception occured: ' + repr(error))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i-pZWSLmYi9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import  f1_score"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfRprrRd8Td7"
      },
      "source": [
        "Use Full Dataset to train and test the Pipeline From First Document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AczjJ4ezmkUy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3a0b5a78-49a1-46af-bb9a-6763ad979ee4"
      },
      "source": [
        "X_full= spam_df['message'].values\n",
        "Y_full= spam_df['label'].values\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
        "    X_full, Y_full, random_state=0)\n",
        "print(f'X_train: {X_train_full.shape} y_train: {y_train_full.shape}')\n",
        "print(f'X_test: {X_test_full.shape} y_test: {y_test_full.shape}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (4179,) y_train: (4179,)\n",
            "X_test: (1393,) y_test: (1393,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31GKEQ7W8e-f"
      },
      "source": [
        "The Pipeline finalized from the mini dataset( ref: file_1) is now used to train the full dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIeoxzAimpLD"
      },
      "source": [
        "vectorization= Pipeline([('preprocessor', SpacyPreprocessor(lammetize= True)), \n",
        "                         ('vectorizer', TfidfVectorizer(analyzer='word', \n",
        "                                                        token_pattern=r\"[\\S]+\", \n",
        "                                                        max_df= 0.8, \n",
        "                                                        max_features= 1000))])\n",
        "\n",
        "feature_engineering = FeatureEngineering(digit_count= True, word_count= True)\n",
        "\n",
        "combined_features = FeatureUnion([('vec', vectorization), \n",
        "                                  ('fe', feature_engineering)]) \n",
        "\n",
        "classifier_final = Pipeline([('cf', combined_features), \n",
        "                             ('classifier', XGBClassifier(scale_pos_weight=7))])\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9xgvlaFmuC0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5f08b8ac-5845-4d45-e098-aaa361592cc3"
      },
      "source": [
        "classifier_final.fit(X_train_full, y_train_full)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('cf',\n",
              "                 FeatureUnion(n_jobs=None,\n",
              "                              transformer_list=[('vec',\n",
              "                                                 Pipeline(memory=None,\n",
              "                                                          steps=[('preprocessor',\n",
              "                                                                  SpacyPreprocessor(add_user_mention_prefix=True,\n",
              "                                                                                    lammetize=True,\n",
              "                                                                                    lower=True,\n",
              "                                                                                    remove_email=True,\n",
              "                                                                                    remove_hashtag_prefix=False,\n",
              "                                                                                    remove_num=False,\n",
              "                                                                                    remove_punct=True,\n",
              "                                                                                    remove_stop=True,\n",
              "                                                                                    remove_url=True,\n",
              "                                                                                    stemming=False)),\n",
              "                                                                 ('vectorizer',\n",
              "                                                                  Tfid...\n",
              "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                               colsample_bylevel=1, colsample_bynode=1,\n",
              "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
              "                               max_delta_step=0, max_depth=3,\n",
              "                               min_child_weight=1, missing=None,\n",
              "                               n_estimators=100, n_jobs=1, nthread=None,\n",
              "                               objective='binary:logistic', random_state=0,\n",
              "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=7,\n",
              "                               seed=None, silent=None, subsample=1,\n",
              "                               verbosity=1))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs_iGtpkmz9h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7cd2234f-1143-4443-e9ab-5e2eae3b2fed"
      },
      "source": [
        "y_pred_train = classifier_final.predict(X_train_full)\n",
        "trainScore_f1 = f1_score(y_train_full, y_pred_train, pos_label= 'spam')\n",
        "y_pred_test = classifier_final.predict(X_test_full)\n",
        "testScore_f1= f1_score(y_test_full, y_pred_test, pos_label= 'spam')\n",
        "print('Train_score: {:.2f}'.format(trainScore_f1))\n",
        "print('Test_score: {:.2f}'.format(testScore_f1))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_score: 0.95\n",
            "Test_score: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FltCZSXc8w6F"
      },
      "source": [
        "This gives us a good Test f1 Score of **0.92** in classifying spam and ham. The train score is 0.95. Now, we will take a look at overall classification report to make sure both ham and spam are predicted correctly.\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKqIg2Jxm3CY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b6a3e069-f102-47a9-b3b6-df20ae249751"
      },
      "source": [
        "from sklearn.metrics import  classification_report\n",
        "print(classification_report(y_test_full,y_pred_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.98      0.99      1196\n",
            "        spam       0.91      0.93      0.92       197\n",
            "\n",
            "    accuracy                           0.98      1393\n",
            "   macro avg       0.95      0.96      0.95      1393\n",
            "weighted avg       0.98      0.98      0.98      1393\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-vY9ajg9Zu4"
      },
      "source": [
        "We Notice that all the scores are above 0.91 which is an indicator of a good classifier. "
      ]
    }
  ]
}