{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mJQGbTFOCAx"
      },
      "source": [
        "<h1 align='center'><u>Spam Detection HW</u></h1>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cwRJe_khRbUJ",
        "outputId": "95826412-a1a4-489b-9794-601ee077482c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXeG6ZQ4OVDj"
      },
      "source": [
        "# Load the dataset  \n",
        "\n",
        "spam dataset from kaggle which can be found from [this](https://www.kaggle.com/uciml/sms-spam-collection-dataset) link. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "12matCKCSOYc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Fk9WNVW-Rj_v",
        "outputId": "26f7ada9-4116-4bb5-91ec-40cd9b77ffd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/Colab_Notebooks/nlpAssignment')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basepath= '/content/drive/MyDrive/Colab_Notebooks/nlpAssignment'\n",
        "folder= Path(basepath)\n",
        "folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kjOvWAAYS3cR",
        "outputId": "268cc789-18b9-40e1-eb40-61a6cb9b1e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "v1,v2,,,\r\n",
            "ham,\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\",,,\r\n",
            "ham,Ok lar... Joking wif u oni...,,,\r\n",
            "spam,Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's,,,\r\n",
            "ham,U dun say so early hor... U c already then say...,,,\r\n",
            "ham,\"Nah I don't think he goes to usf, he lives around here though\",,,\r\n",
            "spam,\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, �1.50 to rcv\",,,\r\n",
            "ham,Even my brother is not like to speak with me. They treat me like aids patent.,,,\r\n",
            "ham,As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune,,,\r\n",
            "spam,WINNER!! As a valued network customer you have been selected to receivea �900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.,,,\r\n"
          ]
        }
      ],
      "source": [
        "# The spam csv has already been downloaded and saved in the data folder.\n",
        "\n",
        "spam = folder / 'spam.csv'\n",
        "!head {spam}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "txlpLjzYVJqx",
        "outputId": "8b1415c8-2931-4538-919a-f2c7cf471aa1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# convert to pandas dataframe\n",
        "spam_file = pd.read_csv(spam,encoding='ISO-8859-1',\n",
        "                        names= ['label','message','1','2','3'],\n",
        "                        usecols=['label','message'], header=0)\n",
        "spam_df = pd.DataFrame(spam_file)\n",
        "spam_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6deSpOJNbOOs",
        "outputId": "ea3394b8-e1ec-4652-fb92-8ba76c0709df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: label, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spam_df.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RnJB0PKNbmVs",
        "outputId": "655244f7-e8d8-442e-d96e-2b186d4501de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.13406317300789664"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "747/(4825+747)    # only 13% is spam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePnOEmVpsQ8V"
      },
      "source": [
        "Only 13% of the data set is \"spam\". The rest 87% is \"ham\". This is an unbalanced dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNdWCViagO4F"
      },
      "source": [
        "# Provide the metric for evaluating model \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK5vxYfcb-Jh"
      },
      "source": [
        "## Evaluation Metric\n",
        "\n",
        "Since this is an unbalanced dataset, classification accuracy is not appropriate here. I choose F1 score as the metric for model evaluation.\n",
        "F1 score is the harmonic mean of precision and recall. It takes both false positive and false negatives into account. Therefore, it performs well on an imbalanced dataset.\n",
        "\n",
        "F1 score conveys the balance between the precision and the recall. Since not only do we need a good recall score (we want spam to be correctly recorded as spam), we also need a good precision score (we also do not want to mislabel ham as spam), F1 score is appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH8_mvGhiThV"
      },
      "source": [
        "# Classification Pipelines\n",
        "\n",
        "    1. Data Preprocessing + Sparse Embeddings (TF-IDF) + ML Model pipeline\n",
        "    2. Feature Engineering + ML Model pipeline\n",
        "    3. Featurization (TF-IDF) + Feature Engineering + ML Model pipeline\n",
        "\n",
        "features can distinguish a spam from a regular email. like Count : Nouns, ProperNouns, AUX, VERBS, Adjectives, named entities, spelling mistakes \n",
        "\n",
        "For Sparse embeddings use **tfidf vectorization**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NK-2OBDgZGu3",
        "outputId": "da0d4b49-97ec-4665-adfa-8d7f602c12b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting thinc<8.1.0,>=8.0.9\n",
            "  Downloading thinc-8.0.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (623 kB)\n",
            "\u001b[K     |████████████████████████████████| 623 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 56.4 MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 50.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.0 pydantic-1.8.2 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iYetX6icZ0rb"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3RnZEACWhJ03"
      },
      "outputs": [],
      "source": [
        "spacy_folder = Path('/content/drive/MyDrive/Colab_Notebooks/nlpAssignment/SPACY')\n",
        "model = spacy_folder /'en_core_web_sm-3.1.0'/'en_core_web_sm'/'en_core_web_sm-3.1.0'\n",
        "nlp = spacy.load(model, disable=['parser'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CF9ejqQvh38g"
      },
      "outputs": [],
      "source": [
        "class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    np.random.seed(0)\n",
        "    def __init__(self, lammetize=True, lower=True, remove_stop=True, \n",
        "                 remove_punct=True, remove_email=True, remove_url=True, \n",
        "                 remove_num=False, stemming = False,\n",
        "                 add_user_mention_prefix=True, remove_hashtag_prefix=False):\n",
        "        self.remove_stop = remove_stop\n",
        "        self.remove_punct = remove_punct\n",
        "        self.remove_num = remove_num\n",
        "        self.remove_url = remove_url\n",
        "        self.remove_email = remove_email\n",
        "        self.lammetize = lammetize\n",
        "        self.lower = lower\n",
        "        self.stemming = stemming\n",
        "        self.add_user_mention_prefix = add_user_mention_prefix\n",
        "        self.remove_hashtag_prefix = remove_hashtag_prefix\n",
        "\n",
        " # helpfer functions for basic cleaning \n",
        "\n",
        "    def basic_clean(self, text):\n",
        "        \n",
        "        '''\n",
        "        This fuction removes HTML tags from text\n",
        "        '''\n",
        "        if (bool(BeautifulSoup(text, \"html.parser\").find())==True):         \n",
        "            soup = BeautifulSoup(text, \"html.parser\")\n",
        "            text = soup.get_text()\n",
        "        else:\n",
        "            pass\n",
        "        return re.sub(r'[\\n\\r]',' ', text) \n",
        "\n",
        "    # helper function for pre-processing with spacy and Porter Stemmer\n",
        "    \n",
        "    def spacy_preprocessor(self,texts):\n",
        "\n",
        "        final_result = []\n",
        "        nlp = spacy.load(model, disable=['parser','ner'])\n",
        "        \n",
        "        ## Add @ as a prefix so that we can separate the word from its token\n",
        "        prefixes = list(nlp.Defaults.prefixes)\n",
        "\n",
        "        if self.add_user_mention_prefix:\n",
        "            prefixes += ['@']\n",
        "\n",
        "        ## Remove # as a prefix so that we can keep hashtags and words together\n",
        "        if self.remove_hashtag_prefix:\n",
        "            prefixes.remove(r'#')\n",
        "\n",
        "        prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n",
        "        nlp.tokenizer.prefix_search = prefix_regex.search\n",
        "\n",
        "        matcher = Matcher(nlp.vocab)\n",
        "        if self.remove_stop:\n",
        "            matcher.add(\"stop_words\", [[{\"is_stop\" : True}]])\n",
        "        if self.remove_punct:\n",
        "            matcher.add(\"punctuation\",[ [{\"is_punct\": True}]])\n",
        "        if self.remove_num:\n",
        "            matcher.add(\"numbers\", [[{\"like_num\": True}]])\n",
        "        if self.remove_url:\n",
        "            matcher.add(\"urls\", [[{\"like_url\": True}]])\n",
        "        if self.remove_email:\n",
        "            matcher.add(\"emails\", [[{\"like_email\": True}]])\n",
        "            \n",
        "        Token.set_extension('is_remove', default=False, force=True)\n",
        "\n",
        "        cleaned_text = []\n",
        "        for doc in nlp.pipe(texts,batch_size=64,disable=['parser','ner']):\n",
        "            matches = matcher(doc)\n",
        "            for _, start, end in matches:\n",
        "                for token in doc[start:end]:\n",
        "                    token._.is_remove =True\n",
        "                    \n",
        "            if self.lammetize:              \n",
        "                text = ' '.join(token.lemma_ for token in doc \n",
        "                                if (token._.is_remove==False))\n",
        "            elif self.stemming:\n",
        "                text =' '.join(PorterStemmer().stem(token.text) for token in doc \n",
        "                                if (token._.is_remove==False))\n",
        "            else:\n",
        "                text = ' '.join(token.text for token in doc \n",
        "                                if (token._.is_remove==False))\n",
        "                                   \n",
        "            if self.lower:\n",
        "                text=text.lower()\n",
        "            cleaned_text.append(text)\n",
        "        return cleaned_text\n",
        "\n",
        "    def fit(self, X,y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        try:\n",
        "            if str(type(X)) not in [\"<class 'list'>\",\"<class 'numpy.ndarray'>\"]:\n",
        "                raise Exception('Expected list or numpy array got {}'.format(type(X)))\n",
        "            x_clean = [self.basic_clean(text) for text in X]\n",
        "            x_clean_final = self.spacy_preprocessor(x_clean)\n",
        "            return x_clean_final\n",
        "        except Exception as error:\n",
        "            print('An exception occured: ' + repr(error))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AIMR3KOJeh07",
        "outputId": "92a279cc-ef42-49a1-baaf-c0fcf618a953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.2-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 4.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.2\n"
          ]
        }
      ],
      "source": [
        "# install pyspellchecker to use in feature engineering \n",
        "\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "8nVuJZGYKOR7"
      },
      "outputs": [],
      "source": [
        "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
        "    np.random.seed(0)\n",
        "    def __init__(self, word_count=False, char_count=False, \n",
        "                 char_count_wo_space=False, \n",
        "                 avg_word_length=True, digit_count=True,\n",
        "                 punc_count= True, spell_check= True):\n",
        "        self.word_count = word_count\n",
        "        self.char_count = char_count\n",
        "        self.char_count_wo_space = char_count_wo_space\n",
        "        self.avg_word_length = avg_word_length\n",
        "        self.digit_count = digit_count\n",
        "        self.punc_count = punc_count\n",
        "        self.spell_check = spell_check\n",
        "        self.spell = SpellChecker()\n",
        "\n",
        "    def fit(self, X,y=None):\n",
        "        return self\n",
        "\n",
        "    #Useful functions\n",
        "\n",
        "    def wordCount(self,text):\n",
        "        return len(text.split())\n",
        "\n",
        "    def charCount(self,text):\n",
        "        return len(text)\n",
        "\n",
        "    def charCountWithoutSpace(self,text):\n",
        "        count = 0\n",
        "        for word in text.split():\n",
        "            count += len(word)\n",
        "        return count\n",
        "\n",
        "    def avgWordLength(self,text):\n",
        "        word_length = 0\n",
        "        for token in text.split():\n",
        "            word_length += len(token)\n",
        "        word_count = len(text.split())\n",
        "        if word_count == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return word_length/word_count\n",
        "\n",
        "    def digitCount(self,text):\n",
        "        count = 0\n",
        "        for i in text:\n",
        "            if i.isdigit():\n",
        "                count += 1\n",
        "        return count\n",
        "    \n",
        "    def punctuationCount(self,text):\n",
        "      line = re.findall(r'[!\"$%&\\'()*+,-./:;=#@?[\\]^_`{|}~]*', text)     \n",
        "      string=\"\".join(line)     \n",
        "      return len(list(string))\n",
        "      # puncList = [token for token in self.nlpdocs if(token.is_punct)]\n",
        "      # return len(puncList)\n",
        "\n",
        "    def spellcheck(self, text,spell):\n",
        "      misspelled= spell.unknown(text.split())\n",
        "      return len(misspelled)\n",
        "\n",
        "    def transform(self, X,y=None):\n",
        "        try:\n",
        "            if str(type(X)) not in [\"<class 'list'>\",\"<class 'numpy.ndarray'>\"]:\n",
        "                raise Exception('Expected list or numpy array got {}'.format(type(X)))\n",
        "            final_result = []\n",
        "            for index,item in enumerate(X):\n",
        "                self.nlpdocs = None\n",
        "                res = []\n",
        "                if self.word_count:\n",
        "                    res.append(self.wordCount(item))\n",
        "                if self.char_count:\n",
        "                    res.append(self.charCount(item))\n",
        "                if self.char_count_wo_space:\n",
        "                    res.append(self.charCountWithoutSpace(item))\n",
        "                if self.avg_word_length:\n",
        "                    res.append(self.avgWordLength(item))\n",
        "                if self.digit_count:\n",
        "                    res.append(self.digitCount(item))\n",
        "                if self.punc_count:\n",
        "                    res.append(self.punctuationCount(item))\n",
        "                if self.spell_check:\n",
        "                    res.append(self.spellcheck(item,self.spell))\n",
        "\n",
        "                final_result.append(res)\n",
        "            return np.array(final_result)\n",
        "        except Exception as error:\n",
        "            print('An exception occured: ' + repr(error))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gzc_uemViTYO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sjmv0uX_RXNP",
        "outputId": "ec38a887-0098-4ef4-eb54-b8c363290118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: (1671,) y_train: (1671,)\n",
            "X_test: (558,) y_test: (558,)\n"
          ]
        }
      ],
      "source": [
        "# 40% sample\n",
        "small_df = pd.DataFrame.sample(spam_df, frac= 0.4,replace=False, random_state=0)\n",
        "small_df.reset_index(drop=True, inplace= True)\n",
        "df = small_df\n",
        "\n",
        "# train test split\n",
        "X= df['message'].values\n",
        "Y= df['label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,random_state=0)\n",
        "print(f'X_train: {X_train.shape} y_train: {y_train.shape}')\n",
        "print(f'X_test: {X_test.shape} y_test: {y_test.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "fWX7L4wRX09g",
        "outputId": "0d69cbce-192f-4d3d-96f8-a906aa975b0c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Aight should I just plan to come up later toni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Was the farm open?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>I sent my scores to sophas and i had to do sec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Was gr8 to see that message. So when r u leavi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>In that case I guess I'll see you at campus lodge</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            message\n",
              "0   ham  Aight should I just plan to come up later toni...\n",
              "1   ham                                 Was the farm open?\n",
              "2   ham  I sent my scores to sophas and i had to do sec...\n",
              "3   ham  Was gr8 to see that message. So when r u leavi...\n",
              "4   ham  In that case I guess I'll see you at campus lodge"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cePKgE38ppOl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer \n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Token\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdVliCHQq52Y"
      },
      "source": [
        "Data preprocessing + Sparse Embeddings + ML Model pipeline :\n",
        "\n",
        "I am using XGBoost Classifier for imbalanced classification. \n",
        "Since the dataset is unbalanced, we will have to apply scale weights for the positive label \"spam\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "RXpKiomSmJPH"
      },
      "outputs": [],
      "source": [
        "# Preprocessing as part of pipeline\n",
        "\n",
        "classifier_1 = Pipeline([('preprocessor', SpacyPreprocessor(remove_stop=False)),\n",
        "                  ('vectorizer', TfidfVectorizer(analyzer='word', \n",
        "                                                 token_pattern=r\"[\\S]+\")),\n",
        "                 \n",
        "                  ('classifier', XGBClassifier(max_depth=4)),\n",
        "                 ])\n",
        "\n",
        "param_grid_classifier_1 = {'preprocessor__lammetize' : [True, False],\n",
        "                'vectorizer__max_features': [500, 1000, 2000],\n",
        "                'vectorizer__max_df': [0.8, 0.7],\n",
        "                'classifier__scale_pos_weight': [7, 13]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "m4ueKalspH2G"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "f1_scorer = make_scorer(f1_score, pos_label=\"spam\")\n",
        "grid_classifier_1 = GridSearchCV(estimator=classifier_1, \n",
        "                                 param_grid=param_grid_classifier_1, \n",
        "                                 cv = 5, scoring= f1_scorer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UnV2evwhp2sJ",
        "outputId": "8f822e88-1394-4635-f11d-129f55c83244"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('preprocessor',\n",
              "                                        SpacyPreprocessor(add_user_mention_prefix=True,\n",
              "                                                          lammetize=True,\n",
              "                                                          lower=True,\n",
              "                                                          remove_email=True,\n",
              "                                                          remove_hashtag_prefix=False,\n",
              "                                                          remove_num=False,\n",
              "                                                          remove_punct=True,\n",
              "                                                          remove_stop=False,\n",
              "                                                          remove_url=True,\n",
              "                                                          stemming=False)),\n",
              "                                       ('vectorizer',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=Fals...\n",
              "                                                      subsample=1,\n",
              "                                                      verbosity=1))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'classifier__scale_pos_weight': [7, 13],\n",
              "                         'preprocessor__lammetize': [True, False],\n",
              "                         'vectorizer__max_df': [0.8, 0.7],\n",
              "                         'vectorizer__max_features': [500, 1000, 2000]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=make_scorer(f1_score, pos_label=spam), verbose=0)"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_classifier_1.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Lhdix2jzEpzw",
        "outputId": "ae3a4831-c1f8-4456-e39b-a505cc6551ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'classifier__scale_pos_weight': 7, 'preprocessor__lammetize': True, 'vectorizer__max_df': 0.8, 'vectorizer__max_features': 500}\n",
            "\n",
            "Best estimator:  Pipeline(memory=None,\n",
            "         steps=[('preprocessor',\n",
            "                 SpacyPreprocessor(add_user_mention_prefix=True, lammetize=True,\n",
            "                                   lower=True, remove_email=True,\n",
            "                                   remove_hashtag_prefix=False,\n",
            "                                   remove_num=False, remove_punct=True,\n",
            "                                   remove_stop=False, remove_url=True,\n",
            "                                   stemming=False)),\n",
            "                ('vectorizer',\n",
            "                 TfidfVectorizer(analyzer='word', binary=False,\n",
            "                                 decode_error='strict',\n",
            "                                 dtype=<class 'numpy.f...\n",
            "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
            "                               colsample_bylevel=1, colsample_bynode=1,\n",
            "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
            "                               max_delta_step=0, max_depth=4,\n",
            "                               min_child_weight=1, missing=None,\n",
            "                               n_estimators=100, n_jobs=1, nthread=None,\n",
            "                               objective='binary:logistic', random_state=0,\n",
            "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=7,\n",
            "                               seed=None, silent=None, subsample=1,\n",
            "                               verbosity=1))],\n",
            "         verbose=False)\n",
            "Train_score: 0.9819639278557115\n",
            "Test_score: 0.8513513513513514\n"
          ]
        }
      ],
      "source": [
        "print(\"Best parameters: {}\".format(grid_classifier_1.best_params_))\n",
        "print(\"\\nBest estimator: \", grid_classifier_1.best_estimator_)\n",
        "\n",
        "print('Train_score: {}'.format(grid_classifier_1.score(X_train,y_train)))\n",
        "print('Test_score: {}'.format(grid_classifier_1.score(X_test,y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dECjDK4uqIni"
      },
      "source": [
        "The Data preprocessing + Sparse Embeddings + ML Model pipeline is giving a decent f1 test score of 0.85. However, the train score is at 0.98 which indicates that there is an overfitting issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS4JmymerGqe"
      },
      "source": [
        "Feature Engineering + ML Model pipeline :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "ASPQNsidVmdj"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering + ML pipeline\n",
        "\n",
        "classifier_2 = Pipeline([('fe', FeatureEngineering()), \n",
        "                         ('classifier', XGBClassifier())])\n",
        "\n",
        "param_grid_classifier_2 = {\n",
        "                'fe__word_count':[True,False],\n",
        "                'fe__digit_count': [True, False],\n",
        "                'classifier__scale_pos_weight': [7, 8, 13]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tEAlcB3TjKUQ",
        "outputId": "4387db4a-b0b1-4e4f-9359-be06d12fcf72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('fe',\n",
              "                                        FeatureEngineering(avg_word_length=True,\n",
              "                                                           char_count=False,\n",
              "                                                           char_count_wo_space=False,\n",
              "                                                           digit_count=True,\n",
              "                                                           punc_count=True,\n",
              "                                                           spell_check=True,\n",
              "                                                           word_count=False)),\n",
              "                                       ('classifier',\n",
              "                                        XGBClassifier(base_score=0.5,\n",
              "                                                      booster='gbtree',\n",
              "                                                      colsample_bylevel=1,\n",
              "                                                      colsample_bynode=1,\n",
              "                                                      colsample_bytree=1,\n",
              "                                                      ga...\n",
              "                                                      reg_alpha=0, reg_lambda=1,\n",
              "                                                      scale_pos_weight=1,\n",
              "                                                      seed=None, silent=None,\n",
              "                                                      subsample=1,\n",
              "                                                      verbosity=1))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'classifier__scale_pos_weight': [7, 8, 13],\n",
              "                         'fe__digit_count': [True, False],\n",
              "                         'fe__word_count': [True, False]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=make_scorer(f1_score, pos_label=spam), verbose=0)"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_classifier_2 = GridSearchCV(estimator = classifier_2, \n",
        "                                 param_grid = param_grid_classifier_2,\n",
        "                                 cv = 5, scoring= f1_scorer)\n",
        "grid_classifier_2.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "m_0-j3nQl7m2",
        "outputId": "e8a89e24-655e-464c-9100-47b94a4a8658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'classifier__scale_pos_weight': 7, 'fe__digit_count': True, 'fe__word_count': False}\n",
            "\n",
            "Best estimator:  Pipeline(memory=None,\n",
            "         steps=[('fe',\n",
            "                 FeatureEngineering(avg_word_length=True, char_count=False,\n",
            "                                    char_count_wo_space=False, digit_count=True,\n",
            "                                    punc_count=True, spell_check=True,\n",
            "                                    word_count=False)),\n",
            "                ('classifier',\n",
            "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
            "                               colsample_bylevel=1, colsample_bynode=1,\n",
            "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
            "                               max_delta_step=0, max_depth=3,\n",
            "                               min_child_weight=1, missing=None,\n",
            "                               n_estimators=100, n_jobs=1, nthread=None,\n",
            "                               objective='binary:logistic', random_state=0,\n",
            "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=7,\n",
            "                               seed=None, silent=None, subsample=1,\n",
            "                               verbosity=1))],\n",
            "         verbose=False)\n",
            "Train_score: 0.9294117647058824\n",
            "Test_score: 0.8789808917197452\n"
          ]
        }
      ],
      "source": [
        "print(\"Best parameters: {}\".format(grid_classifier_2.best_params_))\n",
        "print(\"\\nBest estimator: \", grid_classifier_2.best_estimator_)\n",
        "\n",
        "print('Train_score: {}'.format(grid_classifier_2.score(X_train,y_train)))\n",
        "print('Test_score: {}'.format(grid_classifier_2.score(X_test,y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1BcPuX65Ayt"
      },
      "source": [
        "The Feature Engineering + ML model Pipeline using grid search is giving  an F1 train score of 0.93, and F1 test score of 0.88. Now this result is definitely an improvement over the previous, classifier_1. There is no problem of overfitting here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYvED5qhFdTF"
      },
      "source": [
        "Data preprocesser + Sparse Embeddings + Feature Engineering + ML Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "qjISmtsgmirO"
      },
      "outputs": [],
      "source": [
        "# Data preprocesser + Sparse Embeddings + Feature Engineering + ML Model\n",
        "\n",
        "vectorization= Pipeline([('preprocessor', SpacyPreprocessor(remove_punct=False)),\n",
        "                         ('vectorizer', TfidfVectorizer(analyzer='word',\n",
        "                                                      token_pattern=r\"[\\S]+\"))])\n",
        "feature_engineering = FeatureEngineering()\n",
        "combined_features = FeatureUnion([('vec', vectorization),\n",
        "                                  ('fe', feature_engineering)]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "k8Voscf18g1D"
      },
      "outputs": [],
      "source": [
        "classifier_3 = Pipeline([('cf', combined_features),\n",
        "                         ('classifier', XGBClassifier())])\n",
        "\n",
        "param_grid_classifier_3 = {'cf__vec__preprocessor__lammetize' : [True, False],\n",
        "                'cf__vec__vectorizer__max_features': [500, 1000, 2000],\n",
        "                'cf__vec__vectorizer__max_df': [0.8, 0.7],\n",
        "                'cf__fe__word_count' : [True],\n",
        "                'cf__fe__digit_count': [True],\n",
        "                'classifier__scale_pos_weight': [7, 13]}\n",
        "grid_classifier_3= GridSearchCV(estimator= classifier_3,\n",
        "                                param_grid= param_grid_classifier_3,\n",
        "                                cv = 5, scoring= f1_scorer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "77m_3ml_1279",
        "outputId": "f810af83-9f61-4fe1-e183-58b961962ec5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('cf',\n",
              "                                        FeatureUnion(n_jobs=None,\n",
              "                                                     transformer_list=[('vec',\n",
              "                                                                        Pipeline(memory=None,\n",
              "                                                                                 steps=[('preprocessor',\n",
              "                                                                                         SpacyPreprocessor(add_user_mention_prefix=True,\n",
              "                                                                                                           lammetize=True,\n",
              "                                                                                                           lower=True,\n",
              "                                                                                                           remove_email=True,\n",
              "                                                                                                           remove_hashtag_prefix=False,\n",
              "                                                                                                           remove_num=False,\n",
              "                                                                                                           remove_punct=False,\n",
              "                                                                                                           remove_stop=True,\n",
              "                                                                                                           remove...\n",
              "             param_grid={'cf__fe__digit_count': [True],\n",
              "                         'cf__fe__word_count': [True],\n",
              "                         'cf__vec__preprocessor__lammetize': [True, False],\n",
              "                         'cf__vec__vectorizer__max_df': [0.8, 0.7],\n",
              "                         'cf__vec__vectorizer__max_features': [500, 1000, 2000],\n",
              "                         'classifier__scale_pos_weight': [7, 13]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=make_scorer(f1_score, pos_label=spam), verbose=0)"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_classifier_3.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6U4UBGKM4j7h",
        "outputId": "b23a8ab7-ccbf-457c-8cd5-900646359297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'cf__fe__digit_count': True, 'cf__fe__word_count': True, 'cf__vec__preprocessor__lammetize': True, 'cf__vec__vectorizer__max_df': 0.8, 'cf__vec__vectorizer__max_features': 1000, 'classifier__scale_pos_weight': 7}\n",
            "Train_score: 0.9919354838709677\n",
            "Test_score: 0.9452054794520548\n"
          ]
        }
      ],
      "source": [
        "print(\"Best parameters: {}\".format(grid_classifier_3.best_params_))\n",
        "\n",
        "print('Train_score: {}'.format(grid_classifier_3.score(X_train,y_train)))\n",
        "print('Test_score: {}'.format(grid_classifier_3.score(X_test,y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxQArrv3R5MH"
      },
      "source": [
        "I'm choosing Classifier_3 as it is giving me the best f1 scores: Train score = 0.99 and Test score= 95.\n",
        "Now we are going to train the pipeline using the complete dataset (spam_df). \n",
        "\n",
        "In the next notebook, I'm using the third pipeline with the best parameters obtained in the grid search to train the model on the full data set."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Suman_file1_hw2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
